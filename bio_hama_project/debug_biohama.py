#!/usr/bin/env python
import sys
import os
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, current_dir)

from models.bio_hama.architecture import BioHAMA
from models.bio_hama.meta_router import CognitiveState
import torch

print("=" * 70)
print("Bio-HAMA ì•„í‚¤í…ì²˜ ì§ì ‘ í…ŒìŠ¤íŠ¸")
print("=" * 70)

# ì„¤ì •
config = {
    'vocab_size': 1000,
    'embed_dim': 128,
    'state_dim': 64,
    'num_sub_goals': 5,
    'routing_top_k': 3,
    'module_names': [
        "SocialCognitionModule", "PlanningModule", "MetacognitionModule",
        "EmotionRegulationModule", "AdaptiveMemoryModule", "MultiExpertsModule",
        "SparseAttentionModule", "AttentionControlModule", "MultimodalModule",
        "TerminationModule", "TopologyLearningModule", "EvolutionaryEngineModule"
    ]
}

print("\n1. ëª¨ë¸ ìƒì„±...")
model = BioHAMA(config)
print("âœ“ ëª¨ë¸ ìƒì„± ì„±ê³µ")

print(f"\n2. ìƒì„±ëœ ëª¨ë“ˆ í™•ì¸...")
print(f"  ì´ ëª¨ë“ˆ ìˆ˜: {len(model.cognitive_modules)}")
first_module_name = list(model.cognitive_modules.keys())[0]
first_module = model.cognitive_modules[first_module_name]
print(f"  ì²« ë²ˆì§¸ ëª¨ë“ˆ ({first_module_name}):")
print(f"    {first_module}")

print("\n3. Forward í…ŒìŠ¤íŠ¸...")
input_ids = torch.randint(0, 1000, (4, 10))
state = CognitiveState(
    working_memory=torch.randn(4, 128),
    affective_context=torch.randn(4, 128)
)

model.eval()
try:
    final_output, next_state, logits, activations = model(input_ids, state)
    print("âœ“ Forward ì„±ê³µ!")
    print(f"  ìµœì¢… ì¶œë ¥ shape: {final_output.shape}")
    print(f"  ëª¨ë“ˆ ì •ì±… ë¡œì§“ shape: {logits.shape}")
    print(f"  í™œì„±í™” ê°€ì¤‘ì¹˜ shape: {activations.shape}")
    print(f"  í™œì„±í™”ëœ ëª¨ë“ˆ ìˆ˜: {activations.sum(dim=1)[0].item():.0f}")
    print("\nğŸ‰ Bio-HAMA í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
except Exception as e:
    print(f"âœ— Forward ì‹¤íŒ¨: {e}")
    import traceback
    traceback.print_exc()

print("=" * 70)

